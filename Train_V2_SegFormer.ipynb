{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torch.cuda import amp\n",
    "import timm\n",
    "from pytorch_toolbelt.inference import tta\n",
    "from pytorch_toolbelt import losses as L\n",
    "\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import segmentation_models_pytorch as smp\n",
    "from augmentation import *\n",
    "from monai.inferers import sliding_window_inference\n",
    "from tifffile import imread\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot_aug= A.Compose([\n",
    "            A.ShiftScaleRotate(shift_limit=0., scale_limit=0., rotate_limit= 180,\n",
    "                                            interpolation=cv2.INTER_LINEAR, border_mode=0, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ])\n",
    "\n",
    "def get_train_transform():\n",
    "    return A.Compose([\n",
    "        A.PadIfNeeded(min_height=CFG['window_size'], min_width=CFG['window_size'], p=1),\n",
    "        A.RandomCrop(CFG['window_size'], CFG['window_size'], p=1),\n",
    "        \n",
    "        A.OneOf([\n",
    "            A.HueSaturationValue(hue_shift_limit=100, sat_shift_limit=15, val_shift_limit=10, p=0.7),\n",
    "            A.CLAHE(clip_limit=2, p=0.3),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),\n",
    "        ], p=0.9),\n",
    "        \n",
    "        A.Blur(blur_limit= 2, p=0.3),\n",
    "        A.GaussNoise(p=0.3),\n",
    "        \n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.Transpose(p=0.5),\n",
    "        \n",
    "#         A.OneOf([\n",
    "#             A.GridDistortion(num_steps=5, distort_limit=0.5, p=1.0),\n",
    "#             A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n",
    "#         ], p=0.7),\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.2, rotate_limit= 15,\n",
    "                                        interpolation=cv2.INTER_LINEAR, border_mode=0, p=0.9),\n",
    "        A.CoarseDropout(max_holes=8, max_height=CFG['window_size']//20, max_width=CFG['window_size']//20,\n",
    "                        min_holes=5, fill_value=0, mask_fill_value=0, p=0.5),\n",
    "        ToTensorV2(p=1.0),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_test_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0),\n",
    "    ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_decode(mask_rle, shape, color=1):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo : hi] = color\n",
    "    return img.reshape(shape).T\n",
    "\n",
    "def read_data(data, mode):\n",
    "    ## read img\n",
    "    img_path= data['image_path']\n",
    "    if CFG['domain_shift'] and mode=='train' and np.random.rand()<CFG['domain_shift']:\n",
    "        img_path= img_path.replace('train_images', 'train_images_domain_shift')\n",
    "    try: img= imread(img_path)\n",
    "    except: img= np.array(Image.open(img_path))\n",
    "    \n",
    "    ## read mask\n",
    "    mask= rle_decode(data['rle'], img.shape[:2][::-1])\n",
    "    mask= np.expand_dims(mask, axis= 2)\n",
    "    \n",
    "    ## organ type\n",
    "    organ= data['organ']\n",
    "    return img, mask, organ\n",
    "\n",
    "\n",
    "class custom_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset, transforms,\n",
    "                 mode='valid',\n",
    "                 cutmix=False,\n",
    "                 mixup=False,\n",
    "                 mosaic=False,\n",
    "                 copypaste=False):\n",
    "        self.dataset= dataset\n",
    "        self.transforms= transforms\n",
    "        self.mode= mode\n",
    "        self.cutmix= cutmix\n",
    "        self.mixup= mixup\n",
    "        self.mosaic= mosaic\n",
    "        self.copypaste= copypaste\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        data= self.dataset.loc[index]\n",
    "        img, mask, organ= read_data(data, self.mode)\n",
    "        \n",
    "        ## scale adjust\n",
    "        img_size= min(img.shape[0], img.shape[1])\n",
    "        img_size= img_size//CFG['img_scale']\n",
    "        img_size= CFG['window_size'] if img_size < CFG['window_size'] else img_size\n",
    "        img= cv2.resize(img, (img_size, img_size))\n",
    "        mask= cv2.resize(mask, (img_size, img_size))\n",
    "        mask= np.expand_dims(mask, axis= 2)\n",
    "        \n",
    "        ## augmentation\n",
    "        while True:\n",
    "            aug= self.transforms(image= img, mask= mask)\n",
    "            aug_img, aug_mask= aug['image'], aug['mask']\n",
    "            if aug_mask.numpy().any(): break\n",
    "        img, mask= aug_img, aug_mask\n",
    "        \n",
    "        # use mosaic\n",
    "        if self.mosaic and np.random.rand() >= (1-self.mosaic) and organ!='lung':\n",
    "            img_1= img.permute(1,2,0).numpy()\n",
    "            mask_1= np.array(mask)\n",
    "            imgs, masks= [], []\n",
    "            imgs.append(img_1)\n",
    "            masks.append(mask_1)\n",
    "            for i in range(3):\n",
    "                while True:\n",
    "                    indx= np.random.randint(len(self.dataset))\n",
    "                    data= self.dataset.loc[indx]\n",
    "                    img_2, mask_2, organ_2= read_data(data, self.mode)\n",
    "                    if organ_2!='lung': break\n",
    "                imgs.append(img_2)\n",
    "                masks.append(mask_2)\n",
    "            img, mask= mosaic_aug(imgs[0].shape[0],\n",
    "                                  CFG['window_size'],\n",
    "                                  imgs[0], masks[0],\n",
    "                                  imgs[1], masks[1],\n",
    "                                  imgs[2], masks[2],\n",
    "                                  imgs[3], masks[3])\n",
    "            img= torch.tensor(img, dtype= torch.float32).permute(2,0,1)\n",
    "            \n",
    "        # use mixup\n",
    "        if self.mixup and np.random.rand() >= (1-self.mixup):\n",
    "            img_1= img.permute(1,2,0).numpy()\n",
    "            mask_1= np.array(mask)\n",
    "            indx= np.random.randint(len(self.dataset))\n",
    "            data= self.dataset.loc[indx]\n",
    "            img_2, mask_2= read_data(data)\n",
    "            img, mask= mixup_aug(img_1.shape[0], \n",
    "                                 CFG['window_size'],\n",
    "                                 img_1, mask_1, \n",
    "                                 img_2, mask_2)\n",
    "            img= torch.tensor(img, dtype= torch.float32).permute(2,0,1)\n",
    "            \n",
    "        # use cutmix\n",
    "        if self.cutmix and np.random.rand() >= (1-self.cutmix) and organ!='lung':\n",
    "            img_1= img.permute(1,2,0).numpy()\n",
    "            mask_1= np.array(mask)\n",
    "            while True:\n",
    "                indx= np.random.randint(len(self.dataset))\n",
    "                data= self.dataset.loc[indx]\n",
    "                img_2, mask_2, organ_2= read_data(data, self.mode)\n",
    "                if organ_2!='lung': break\n",
    "            img, mask= cutmix_aug(img_1.shape[0], \n",
    "                                  CFG['window_size'],\n",
    "                                  img_1, mask_1, \n",
    "                                  img_2, mask_2)\n",
    "            img= torch.tensor(img, dtype= torch.float32).permute(2,0,1)\n",
    "            \n",
    "        # use copypaste\n",
    "        if self.copypaste and np.random.rand() >= (1-self.copypaste):\n",
    "            img_1= img.permute(1,2,0).numpy()\n",
    "            mask_1= np.array(mask)\n",
    "            while True:\n",
    "                indx= np.random.randint(len(self.dataset))\n",
    "                data= self.dataset.loc[indx]\n",
    "                img_2, mask_2= read_data(data)\n",
    "                if mask_2.any(): break\n",
    "            img, mask= copy_paste(img_1.shape[0], \n",
    "                                  CFG['window_size'],\n",
    "                                  img_1, mask_1, \n",
    "                                  img_2, mask_2)\n",
    "            img= torch.tensor(img, dtype= torch.float32).permute(2,0,1)\n",
    "\n",
    "        ## rot aug\n",
    "        img= img.permute(1,2,0).numpy()\n",
    "        mask= np.array(mask)\n",
    "        aug= rot_aug(image= img, mask= mask)\n",
    "        img, mask= aug['image'], aug['mask']\n",
    "            \n",
    "        mask= torch.tensor(mask, dtype= torch.float)\n",
    "        mask= mask.permute(2,0,1)\n",
    "\n",
    "        ## aux label\n",
    "        if CFG['aux']:\n",
    "            aux_label= np.zeros((len(mask),))\n",
    "            for i in range(len(mask)):\n",
    "                if mask[i].numpy().any(): \n",
    "                    aux_label[i]= 1\n",
    "        else:\n",
    "            aux_label= [-1]\n",
    "            \n",
    "        if CFG['amp']: return img/255, mask, aux_label\n",
    "        else: return img/255, mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_loss(nn.Module):\n",
    "    def  __init__(self):\n",
    "        super().__init__()\n",
    "        self.JaccardLoss = smp.losses.JaccardLoss(mode='multilabel')\n",
    "        self.DiceLoss    = smp.losses.DiceLoss(mode='binary', from_logits=True)\n",
    "        self.BCELoss     = smp.losses.SoftBCEWithLogitsLoss()\n",
    "        self.LovaszLoss  = smp.losses.LovaszLoss(mode='multilabel', per_image=False)\n",
    "        self.TverskyLoss = smp.losses.TverskyLoss(mode='multilabel', log_loss=False, from_logits=True)\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss= 0.5*self.DiceLoss(y_pred, y_true) + 0.5*self.BCELoss(y_pred, y_true)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_amp(dataloader, model, criterion, optimizer):\n",
    "    scaler= amp.GradScaler()\n",
    "    model.train()\n",
    "\n",
    "    ep_loss= []\n",
    "    for i, (imgs, masks, aux_labels) in enumerate(tqdm(dataloader)):\n",
    "\n",
    "        imgs= imgs.to('cuda')\n",
    "        masks= masks.to('cuda')\n",
    "        if CFG['aux']: aux_labels= aux_labels.to('cuda')\n",
    "        \n",
    "        with amp.autocast():\n",
    "            preds= model(imgs)\n",
    "            if CFG['aux']:\n",
    "                loss_1= criterion(preds[0], masks)\n",
    "                loss_2= criterion(preds[1], aux_labels)\n",
    "                loss= (1-CFG['aux'])*loss_1 + CFG['aux']*loss_2\n",
    "            else:\n",
    "                loss= criterion(preds, masks)\n",
    "            ep_loss.append(loss.item())\n",
    "            loss/= CFG['gradient_accumulation']\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            if (i+1) % CFG['gradient_accumulation']== 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "    return np.mean(ep_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch_amp(dataloader, model, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    ep_loss= []\n",
    "    masks_dice= []\n",
    "    auxs_dice= []\n",
    "    for i, (imgs, masks, aux_label) in enumerate(tqdm(dataloader)):\n",
    "\n",
    "        imgs= imgs.to('cuda')\n",
    "        masks= masks.to('cuda')\n",
    "        if CFG['aux']: aux_label= aux_label.to('cuda')\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            preds= sliding_window_inference(imgs, \n",
    "                                            (CFG['window_size'], CFG['window_size']), \n",
    "                                            sw_batch_size= 2, \n",
    "                                            predictor= model, \n",
    "                                            mode= 'gaussian',\n",
    "                                            overlap= 0.25)\n",
    "            if CFG['aux']:\n",
    "                loss_1= criterion(preds[0], masks)\n",
    "                loss_2= criterion(preds[1], aux_label)\n",
    "                loss= (1-CFG['aux'])*loss_1 + CFG['aux']*loss_2\n",
    "            else:\n",
    "                loss= criterion(preds, masks)\n",
    "            ep_loss.append(loss.item())\n",
    "            \n",
    "        ## evaluation\n",
    "        if CFG['aux']: pred_mask= preds[0].cpu()\n",
    "        else: pred_mask= preds.cpu()\n",
    "        mask_dice= dice( pred_mask, masks.cpu() )\n",
    "        masks_dice.append(mask_dice)\n",
    "            \n",
    "        if CFG['aux']:\n",
    "            pred_aux= preds[1].cpu()\n",
    "            aux_dice= dice( pred_aux, aux_label.cpu() )\n",
    "            auxs_dice.append(aux_dice)\n",
    "        \n",
    "    ## metrice\n",
    "    dice_mask= 1 - np.mean(masks_dice)\n",
    "    dice_aux= 1 - np.mean(auxs_dice) if CFG['aux'] else None\n",
    "        \n",
    "    return np.mean(ep_loss), dice_mask, dice_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "class customize_model(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(customize_model, self).__init__()\n",
    "        \n",
    "        self.model = SegformerForSemanticSegmentation.from_pretrained(model_name)\n",
    "        self.model.decode_head.classifier= nn.Sequential(\n",
    "                                                nn.Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                                nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "                                                nn.Conv2d(384, 1, kernel_size=(1, 1), stride=(1, 1)),\n",
    "                                                nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "                                            )\n",
    "        \n",
    "    def forward(self, images):\n",
    "        out= self.model(images)['logits']\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG= {\n",
    "    'fold': 0,\n",
    "    'epoch': 150,\n",
    "    'img_scale': 3,\n",
    "    'window_size': 1024,\n",
    "    'model_name': 'nvidia/segformer-b5-finetuned-ade-640-640',\n",
    "    'aux': False,\n",
    "    'classes_balance': True,\n",
    "    'finetune': False,\n",
    "    'domain_shift': 0.5,\n",
    "    \n",
    "    'lr': 6e-5,\n",
    "    'weight_decay': 0,\n",
    "    'batch_size': 1,\n",
    "    'gradient_accumulation': 8,\n",
    "    'amp': True,\n",
    "    \n",
    "    'cutmix': 0.5,\n",
    "    'mixup': False,\n",
    "    'mosaic': 0.5,\n",
    "    'copypaste': False,\n",
    "    \n",
    "    'load_model': False,\n",
    "    'save_model': 'train_model',\n",
    "}\n",
    "if CFG['aux']!=False: CFG['amp']= True\n",
    "if CFG['finetune']:\n",
    "    print('finetune')\n",
    "    CFG['epoch']= 20\n",
    "    CFG['lr']= 3e-5\n",
    "    CFG['load_model']= f\"train_model/model_cv{CFG['fold']}_best.pth\"\n",
    "    \n",
    "# CFG['load_model']= f\"train_model/model_cv{CFG['fold']}_best.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('./Data/train.csv')\n",
    "# ex_df= pd.read_csv('./Data/train_ex_largeintestine.csv').fillna('')\n",
    "# ex_df= ex_df[ex_df['rle']!='']\n",
    "# ex_df['fold']= -1\n",
    "# df= pd.concat([ex_df, df], axis=0)\n",
    "\n",
    "## choose organ\n",
    "choose_organ= [\n",
    "    'lung',\n",
    "    'spleen',\n",
    "    'prostate',\n",
    "    'kidney',\n",
    "    'largeintestine'\n",
    "]\n",
    "df= df[df['organ'].isin(choose_organ)]\n",
    "df.loc[ df['organ']=='lung', 'fold' ]= -1\n",
    "\n",
    "train_df= df[df['fold']!=CFG['fold']].reset_index()\n",
    "valid_df= df[df['fold']==CFG['fold']].reset_index()\n",
    "\n",
    "## classes balance\n",
    "if CFG['classes_balance']:\n",
    "    print('balance classes')\n",
    "    max_sample= len(train_df[train_df['organ']=='kidney'])\n",
    "    for organ in choose_organ:\n",
    "        fill_num= max_sample - len(train_df[train_df['organ']==organ])\n",
    "        if fill_num:\n",
    "            df= train_df[train_df['organ']==organ].reset_index(drop=True)\n",
    "            try: sample_df= df.sample(n= fill_num, replace= False)\n",
    "            except: sample_df= df.sample(n= fill_num, replace= True)\n",
    "            train_df= pd.concat([train_df, sample_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "print(f'train dataset: {len(train_df)}')\n",
    "print(f'valid dataset: {len(valid_df)}')\n",
    "\n",
    "train_dataset= custom_dataset(train_df,\n",
    "                              get_train_transform(),\n",
    "                              mode= 'train',\n",
    "                              cutmix= CFG['cutmix'],\n",
    "                              mixup= CFG['mixup'],\n",
    "                              mosaic= CFG['mosaic'],\n",
    "                              copypaste= CFG['copypaste'])\n",
    "valid_dataset= custom_dataset(valid_df, get_test_transform())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size= CFG['batch_size'], shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size= 1, shuffle=False, num_workers=0)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_dataset= custom_dataset(train_df,\n",
    "                              get_train_transform(),\n",
    "                              mode= 'train',\n",
    "                              cutmix= CFG['cutmix'],\n",
    "                              mixup= CFG['mixup'],\n",
    "                              mosaic= CFG['mosaic'],\n",
    "                              copypaste= CFG['copypaste'])\n",
    "\n",
    "data= train_dataset[3]\n",
    "img= data[0].permute(1,2,0).numpy()\n",
    "mask= data[1].permute(1,2,0).numpy()\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:0'\n",
    "\n",
    "if not CFG['load_model']:\n",
    "    model= customize_model(CFG['model_name'])\n",
    "else:\n",
    "    print(f\"load_model: {CFG['load_model']}\")\n",
    "    model= torch.load(CFG['load_model'])\n",
    "\n",
    "## loss\n",
    "loss= custom_loss()\n",
    "loss.__name__ = 'custom_loss'\n",
    "dice= L.DiceLoss(mode= 'multilabel')\n",
    "dice.__name__= 'dice_loss'\n",
    "\n",
    "metrics = [\n",
    "#     smp.utils.metrics.Fscore(threshold= 0.5, activation= 'sigmoid'),\n",
    "    smp.utils.metrics.IoU(threshold=0.5, activation= 'sigmoid'),\n",
    "    dice,\n",
    "]\n",
    "optimizer = torch.optim.AdamW([ \n",
    "    dict(params=model.parameters(), lr=CFG['lr']),\n",
    "], weight_decay= CFG['weight_decay'])\n",
    "\n",
    "\n",
    "model.train()\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "max_score = 0\n",
    "for i in range(1, CFG['epoch']+1):\n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    \n",
    "    ## train\n",
    "    if CFG['amp']: \n",
    "        train_loss= train_epoch_amp(train_loader, model, loss, optimizer)\n",
    "        valid_loss, dice_mask, dice_aux= valid_epoch_amp(valid_loader, model, loss)\n",
    "        score= dice_mask\n",
    "        print(f'train_loss: {round(train_loss, 5)}')\n",
    "        print(f'valid_loss: {round(valid_loss, 5)},\\\n",
    "                vali_score: {round(score, 5)},\\\n",
    "                valid_aux: {dice_aux}')\n",
    "    else: \n",
    "        train_logs = train_epoch.run(train_loader)\n",
    "        valid_logs = valid_epoch.run(valid_loader)\n",
    "        score= 1-valid_logs['dice_loss']\n",
    "        print('vali_score: ', round(score, 5))\n",
    "\n",
    "    ## save best model\n",
    "    if max_score < score:\n",
    "        max_score = score\n",
    "        torch.save(model, f\"{CFG['save_model']}/model_cv{CFG['fold']}_best.pth\")\n",
    "        print('Model saved at: ', round(max_score, 5))\n",
    "        \n",
    "    ## save model every epoch\n",
    "    torch.save(model, f\"{CFG['save_model']}/model_cv{CFG['fold']}_ep{i}.pth\")\n",
    "\n",
    "#     ##adjust lr\n",
    "#     if i == 70:\n",
    "#         model= torch.load(f\"{CFG['save_model']}/model_cv{CFG['fold']}_best.pth\")\n",
    "#         optimizer.param_groups[0]['lr'] = 1e-4\n",
    "#         print(f\"Decrease decoder learning rate to {optimizer.param_groups[0]['lr']}!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
