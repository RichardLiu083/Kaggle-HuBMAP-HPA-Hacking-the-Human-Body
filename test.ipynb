{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c321d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from monai.inferers import sliding_window_inference\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51a5fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_rgb_img(img_path):\n",
    "    img= np.array(Image.open(img_path))\n",
    "    orign_shape= img.shape[:2][::-1]\n",
    "    return img, orign_shape\n",
    "\n",
    "def get_test_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0),\n",
    "    ])\n",
    "def get_pad_transform(img_size):\n",
    "    return A.Compose([\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, p=1),\n",
    "    ])\n",
    "def get_crop_transform(img_size):\n",
    "    return A.Compose([\n",
    "        A.CenterCrop(img_size, img_size, p=1),\n",
    "    ])\n",
    "\n",
    "class Customize_Dataset(Dataset):\n",
    "    def __init__(self, df, transforms):\n",
    "        self.df = df\n",
    "        self.image_path = df['image_path'].values\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.image_path[index]\n",
    "        img, ori_shape= read_rgb_img(img_path)\n",
    "        \n",
    "        ## scale adjust\n",
    "        img_size= min(img.shape[0], img.shape[1])\n",
    "        img_size= img_size/CFG['img_scale']\n",
    "        img_size= int(img_size)\n",
    "        img= cv2.resize(img, (img_size, img_size))\n",
    "        \n",
    "        if img.shape[0]<CFG['window_size']:\n",
    "            pad= img.shape[0]\n",
    "            aug= get_pad_transform(CFG['window_size'])\n",
    "            img= aug(image= img)['image']\n",
    "        else:\n",
    "            pad= 0\n",
    "        \n",
    "        img = self.transforms(image=img)[\"image\"]\n",
    "        return {\n",
    "            'img_path': img_path,\n",
    "            'image': torch.tensor(img/255, dtype=torch.float32),\n",
    "            'ori_shape': torch.tensor(ori_shape),\n",
    "            'pad': pad,\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99041d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class customize_model(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(customize_model, self).__init__()\n",
    "        \n",
    "    def forward(self, images):\n",
    "        out= self.model(images)['logits']\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec968a6",
   "metadata": {},
   "source": [
    "# CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff80862",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG= {\n",
    "    'fold': 0,\n",
    "    'img_scale': (3/(0.4945/0.4)),\n",
    "    'window_size': 1024,\n",
    "    'TTA': False,\n",
    "    'model': None,\n",
    "    'show_result': False,\n",
    "}\n",
    "CFG['model']= f\"./train_model/model_cv{CFG['fold']}_best.pth\"\n",
    "# CFG['model']= f\"./train_model/model_cv{CFG['fold']}_ep99.pth\"\n",
    "# CFG['model']= f\"./test_model/effb7_w768_cv0_best/model_cv{CFG['fold']}_best.pth\"\n",
    "# CFG['model']= f\"./test_model/effb7_w768_0.49\"\n",
    "\n",
    "CFG['model']= [torch.load(CFG['model'], map_location= 'cuda:0')]\n",
    "# CFG['model']= [torch.load(m, map_location= 'cuda:0') for m in glob.glob(f\"{CFG['model']}/**/*pth\", recursive=True)[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ddded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('./Data/sample_submission.csv')\n",
    "df['image_path']= None\n",
    "for i in range(len(df)):\n",
    "    id_= df.loc[i, ['id']].values[0]\n",
    "    path= f'./Data/test_images/{id_}.tiff'\n",
    "    df.loc[i, ['image_path']]= path\n",
    "    \n",
    "test_dataset= Customize_Dataset(df, get_test_transform())\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87256076",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722acd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle_encode(img):\n",
    "    #the image should be transposed\n",
    "    pixels = img.T.flatten()\n",
    "    # This simplified method requires first and last pixel to be zero\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def inference(model, img):\n",
    "    img= img.cuda()\n",
    "    for i, m in enumerate(model):\n",
    "        with torch.no_grad():\n",
    "            m.eval()\n",
    "            if CFG['TTA']:\n",
    "                imgs= torch.cat([img, \n",
    "                                 img.rot90(1, [2,3]),\n",
    "                                 img.rot90(2, [2,3]),\n",
    "                                 img.rot90(3, [2,3]),\n",
    "                                 img.flip(-1), \n",
    "                                 img.flip(-1).rot90(1, [2,3]),\n",
    "                                 img.flip(-1).rot90(2, [2,3]),\n",
    "                                 img.flip(-1).rot90(3, [2,3])], dim=0)\n",
    "                pred= sliding_window_inference(imgs, \n",
    "                                               (CFG['window_size'], CFG['window_size']), \n",
    "                                               sw_batch_size= 2, \n",
    "                                               predictor= m,\n",
    "                                               mode= 'gaussian',\n",
    "                                               overlap= 0.25)\n",
    "                pred= (pred[0] + \n",
    "                       pred[1].rot90(-1, [1,2]) +\n",
    "                       pred[2].rot90(-2, [1,2]) +\n",
    "                       pred[3].rot90(-3, [1,2]) +\n",
    "                       pred[4].flip(-1) + \n",
    "                       pred[5].rot90(-1, [1,2]).flip(-1) + \n",
    "                       pred[6].rot90(-2, [1,2]).flip(-1) + \n",
    "                       pred[7].rot90(-3, [1,2]).flip(-1)) / 8\n",
    "            else:\n",
    "                pred= sliding_window_inference(img, \n",
    "                                               (CFG['window_size'], CFG['window_size']), \n",
    "                                               sw_batch_size= 2, \n",
    "                                               predictor= m,\n",
    "                                               mode= 'gaussian',\n",
    "                                               overlap= 0.25)[0]\n",
    "        if pred.shape[0]!=1:\n",
    "            if i==0: preds= pred.softmax(dim=0)\n",
    "            else: preds+= pred.softmax(dim=0)\n",
    "        else:\n",
    "            if i==0: preds= pred.sigmoid()\n",
    "            else: preds+= pred.sigmoid()\n",
    "    pred= preds/len(model)\n",
    "    pred= pred.cpu().permute(1,2,0).numpy()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652f047f",
   "metadata": {},
   "outputs": [],
   "source": [
    "indx= 0\n",
    "for i, data in enumerate(tqdm(test_loader)):\n",
    "    for j in range(len(data['image'])):\n",
    "        img_path= data['img_path'][j]\n",
    "        img= data['image'][j]\n",
    "        ori_shape= data['ori_shape'][j].numpy()\n",
    "        pad= data['pad'][j]\n",
    "        \n",
    "        ## inference\n",
    "        img= torch.unsqueeze(img, dim= 0)\n",
    "        pred_mask= inference(CFG['model'], img)\n",
    "        \n",
    "        if pred_mask.shape[2]!=1:\n",
    "            pred_mask= pred_mask[..., 2]\n",
    "        \n",
    "        ## if pad\n",
    "        if pad!=0:\n",
    "            aug= get_crop_transform(pad)\n",
    "            pred_mask= aug(image= pred_mask)['image']\n",
    "            pred_mask= cv2.resize(pred_mask, tuple(ori_shape))\n",
    "        else:\n",
    "            pred_mask= cv2.resize(pred_mask, tuple(ori_shape))\n",
    "        \n",
    "        thr= 0.1\n",
    "        pred_mask[pred_mask>=thr]= 1\n",
    "        pred_mask[pred_mask<thr]= 0\n",
    "        pred_mask= pred_mask.astype(np.uint8)\n",
    "        \n",
    "        sample_img= img[0].permute(1,2,0).numpy()\n",
    "        if pad!=0:\n",
    "            aug= get_crop_transform(pad)\n",
    "            sample_img= aug(image= sample_img)['image']\n",
    "            sample_img= cv2.resize(sample_img, tuple(ori_shape))\n",
    "        else:\n",
    "            sample_img= cv2.resize(sample_img, tuple(ori_shape))\n",
    "        plt.imshow(sample_img)\n",
    "        plt.show()\n",
    "        plt.imshow(pred_mask)\n",
    "        plt.show()\n",
    "        rle= rle_encode(pred_mask)\n",
    "        df.loc[indx, 'rle']= rle\n",
    "        indx+= 1\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45872bb0",
   "metadata": {},
   "source": [
    "for i in range(6):\n",
    "    plt.imshow(pred_mask[...,i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "412544b9",
   "metadata": {},
   "source": [
    "import pretrained.zoo as zoo\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "model= zoo.TimmUnet(encoder='tf_efficientnetv2_s_in21k', in_chans=3, pretrained=True)\n",
    "x= torch.rand(2,3,512,512)\n",
    "out= model(x)\n",
    "out.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
