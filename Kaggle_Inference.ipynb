{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f5878c5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-09-21T13:13:27.624892Z",
     "iopub.status.busy": "2022-09-21T13:13:27.624412Z",
     "iopub.status.idle": "2022-09-21T13:18:13.136580Z",
     "shell.execute_reply": "2022-09-21T13:18:13.135511Z"
    },
    "papermill": {
     "duration": 285.525941,
     "end_time": "2022-09-21T13:18:13.144141",
     "exception": false,
     "start_time": "2022-09-21T13:13:27.618200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/smp-021/SMP_0.2.1/efficientnet_pytorch-0.6.3-py3-none-any.whl\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet-pytorch==0.6.3) (1.11.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->efficientnet-pytorch==0.6.3) (4.1.1)\r\n",
      "Installing collected packages: efficientnet-pytorch\r\n",
      "Successfully installed efficientnet-pytorch-0.6.3\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/smp-021/SMP_0.2.1/pretrainedmodels-0.7.4-py3-none-any.whl\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4) (4.64.0)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4) (1.11.0)\r\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4) (2.5.0)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4) (0.12.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from munch->pretrainedmodels==0.7.4) (1.16.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch->pretrainedmodels==0.7.4) (4.1.1)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->pretrainedmodels==0.7.4) (9.1.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->pretrainedmodels==0.7.4) (2.27.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->pretrainedmodels==0.7.4) (1.21.6)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->pretrainedmodels==0.7.4) (2022.6.15)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->pretrainedmodels==0.7.4) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->pretrainedmodels==0.7.4) (2.0.12)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->pretrainedmodels==0.7.4) (1.26.9)\r\n",
      "Installing collected packages: pretrainedmodels\r\n",
      "Successfully installed pretrainedmodels-0.7.4\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/smp-021/SMP_0.2.1/timm-0.4.12-py3-none-any.whl\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm==0.4.12) (0.12.0)\r\n",
      "Requirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.7/site-packages (from timm==0.4.12) (1.11.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.4->timm==0.4.12) (4.1.1)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.4.12) (9.1.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.4.12) (1.21.6)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision->timm==0.4.12) (2.27.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm==0.4.12) (2022.6.15)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm==0.4.12) (2.0.12)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm==0.4.12) (1.26.9)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision->timm==0.4.12) (3.3)\r\n",
      "Installing collected packages: timm\r\n",
      "Successfully installed timm-0.4.12\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/smp-021/SMP_0.2.1/segmentation_models_pytorch-0.2.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: pretrainedmodels==0.7.4 in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch==0.2.1) (0.7.4)\r\n",
      "Requirement already satisfied: timm==0.4.12 in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch==0.2.1) (0.4.12)\r\n",
      "Requirement already satisfied: efficientnet-pytorch==0.6.3 in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch==0.2.1) (0.6.3)\r\n",
      "Requirement already satisfied: torchvision>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from segmentation-models-pytorch==0.2.1) (0.12.0)\r\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.1) (1.11.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.1) (4.64.0)\r\n",
      "Requirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.1) (2.5.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.2.1) (1.21.6)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.2.1) (9.1.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.2.1) (4.1.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.2.1) (2.27.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.1) (1.16.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch==0.2.1) (1.26.9)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch==0.2.1) (2.0.12)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch==0.2.1) (2022.6.15)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision>=0.5.0->segmentation-models-pytorch==0.2.1) (3.3)\r\n",
      "Installing collected packages: segmentation-models-pytorch\r\n",
      "Successfully installed segmentation-models-pytorch-0.2.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/transformers4162/transformers-4.16.2-py3-none-any.whl\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.2) (0.7.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.2) (2021.11.10)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.2) (2.27.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.2) (6.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.2) (0.12.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.2) (3.6.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.2) (21.3)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.2) (4.11.4)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.2) (0.0.53)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.2) (1.21.6)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==4.16.2) (4.64.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.2) (4.1.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.16.2) (3.0.9)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.16.2) (3.8.0)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.16.2) (2.0.12)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.16.2) (2022.6.15)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.16.2) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==4.16.2) (1.26.9)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.16.2) (1.16.0)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.16.2) (1.1.0)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.16.2) (8.0.4)\r\n",
      "Installing collected packages: transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.18.0\r\n",
      "    Uninstalling transformers-4.18.0:\r\n",
      "      Successfully uninstalled transformers-4.18.0\r\n",
      "Successfully installed transformers-4.16.2\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/torch-171/torch-1.7.1cu101-cp37-cp37m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.7.1cu101) (1.21.6)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.7.1cu101) (4.1.1)\r\n",
      "Installing collected packages: torch\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 1.11.0\r\n",
      "    Uninstalling torch-1.11.0:\r\n",
      "      Successfully uninstalled torch-1.11.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "pytorch-lightning 1.6.4 requires torch>=1.8.*, but you have torch 1.7.1+cu101 which is incompatible.\r\n",
      "fairscale 0.4.6 requires torch>=1.8.0, but you have torch 1.7.1+cu101 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed torch-1.7.1+cu101\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/torchvision/torchvision-0.8.2cu101-cp37-cp37m-linux_x86_64.whl\r\n",
      "Requirement already satisfied: torch==1.7.1 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.8.2cu101) (1.7.1+cu101)\r\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision==0.8.2cu101) (9.1.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==0.8.2cu101) (1.21.6)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.7.1->torchvision==0.8.2cu101) (4.1.1)\r\n",
      "Installing collected packages: torchvision\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.12.0\r\n",
      "    Uninstalling torchvision-0.12.0:\r\n",
      "      Successfully uninstalled torchvision-0.12.0\r\n",
      "Successfully installed torchvision-0.8.2+cu101\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/einop-041/einops-0.4.1-py3-none-any.whl\r\n",
      "Installing collected packages: einops\r\n",
      "Successfully installed einops-0.4.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCPU times: user 8.25 s, sys: 2.06 s, total: 10.3 s\n",
      "Wall time: 4min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pip install ../input/smp-021/SMP_0.2.1/efficientnet_pytorch-0.6.3-py3-none-any.whl\n",
    "!pip install ../input/smp-021/SMP_0.2.1/pretrainedmodels-0.7.4-py3-none-any.whl\n",
    "!pip install ../input/smp-021/SMP_0.2.1/timm-0.4.12-py3-none-any.whl\n",
    "!pip install ../input/smp-021/SMP_0.2.1/segmentation_models_pytorch-0.2.1-py3-none-any.whl\n",
    "!pip install ../input/transformers4162/transformers-4.16.2-py3-none-any.whl\n",
    "!pip install ../input/torch-171/torch-1.7.1cu101-cp37-cp37m-linux_x86_64.whl\n",
    "!pip install ../input/torchvision/torchvision-0.8.2cu101-cp37-cp37m-linux_x86_64.whl\n",
    "!pip install ../input/einop-041/einops-0.4.1-py3-none-any.whl\n",
    "\n",
    "import sys\n",
    "sys.path.append('../input/monai-v081/')\n",
    "import monai\n",
    "from monai.inferers import sliding_window_inference\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as F\n",
    "from tqdm import tqdm\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import gc\n",
    "import glob\n",
    "from PIL import Image\n",
    "from tifffile import imread\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage import measure\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "def seed_everything(seed=123):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3da2a280",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-21T13:18:13.159042Z",
     "iopub.status.busy": "2022-09-21T13:18:13.157885Z",
     "iopub.status.idle": "2022-09-21T13:18:13.171442Z",
     "shell.execute_reply": "2022-09-21T13:18:13.170597Z"
    },
    "papermill": {
     "duration": 0.023038,
     "end_time": "2022-09-21T13:18:13.173459",
     "exception": false,
     "start_time": "2022-09-21T13:18:13.150421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_rgb_img(img_path):\n",
    "    img= imread(img_path)\n",
    "    orign_shape= img.shape[:2][::-1]\n",
    "    return img, orign_shape\n",
    "\n",
    "def get_test_transform():\n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0),\n",
    "    ])\n",
    "def get_pad_transform(img_size):\n",
    "    return A.Compose([\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, p=1),\n",
    "    ])\n",
    "def get_crop_transform(img_size):\n",
    "    return A.Compose([\n",
    "        A.CenterCrop(img_size, img_size, p=1),\n",
    "    ])\n",
    "\n",
    "class Customize_Dataset(Dataset):\n",
    "    def __init__(self, df, scale, img_size, transforms):\n",
    "        self.df = df\n",
    "        self.scale = scale\n",
    "        self.img_size = img_size\n",
    "        self.image_path = df['image_path'].values\n",
    "        self.organ_type = df['organ'].values\n",
    "        self.pixel_size = df['pixel_size'].values\n",
    "        self.data_source= df['data_source'].values\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.image_path[index]\n",
    "        organ= self.organ_type[index]\n",
    "        p_size= self.pixel_size[index]\n",
    "        d_source= self.data_source[index]\n",
    "        img, ori_shape= read_rgb_img(img_path)\n",
    "        \n",
    "        ## scale adjust\n",
    "        scale= float(p_size)/0.4/self.scale\n",
    "        img_size= min(img.shape[0], img.shape[1])\n",
    "        img_size= int(img_size * scale)\n",
    "        img= cv2.resize(img, (img_size, img_size))\n",
    "        \n",
    "        ## need padding\n",
    "        if img.shape[0]<self.img_size:\n",
    "            pad= img.shape[0]\n",
    "            aug= get_pad_transform(self.img_size)\n",
    "            img= aug(image= img)['image']\n",
    "        else:\n",
    "            pad= 0\n",
    "        \n",
    "        img = self.transforms(image=img)[\"image\"]\n",
    "        return {\n",
    "            'img_path': img_path,\n",
    "            'image': torch.tensor(img/255, dtype=torch.float32),\n",
    "            'ori_shape': torch.tensor(ori_shape),\n",
    "            'pad': pad,\n",
    "            'organ': organ,\n",
    "            'data_source': d_source,\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8528e66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-21T13:18:13.186362Z",
     "iopub.status.busy": "2022-09-21T13:18:13.186097Z",
     "iopub.status.idle": "2022-09-21T13:18:13.220007Z",
     "shell.execute_reply": "2022-09-21T13:18:13.219179Z"
    },
    "papermill": {
     "duration": 0.042798,
     "end_time": "2022-09-21T13:18:13.222043",
     "exception": false,
     "start_time": "2022-09-21T13:18:13.179245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.append('../input/coat-net')\n",
    "from coat import *\n",
    "from daformer import *\n",
    "from helper import *\n",
    "from coatnet_inference import *\n",
    "\n",
    "class customize_model(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(customize_model, self).__init__()\n",
    "        \n",
    "    def forward(self, images):\n",
    "        out= self.model(images)['logits']\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd274829",
   "metadata": {
    "papermill": {
     "duration": 0.005637,
     "end_time": "2022-09-21T13:18:13.233622",
     "exception": false,
     "start_time": "2022-09-21T13:18:13.227985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d72cc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-21T13:18:13.247182Z",
     "iopub.status.busy": "2022-09-21T13:18:13.246312Z",
     "iopub.status.idle": "2022-09-21T13:18:13.253403Z",
     "shell.execute_reply": "2022-09-21T13:18:13.252536Z"
    },
    "papermill": {
     "duration": 0.016049,
     "end_time": "2022-09-21T13:18:13.255480",
     "exception": false,
     "start_time": "2022-09-21T13:18:13.239431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFG= {\n",
    "    ## model for 4 types organ\n",
    "    'preprocessing': [\n",
    "        's4_w640',\n",
    "        's4_w768',\n",
    "        's3_w1024',\n",
    "        \n",
    "        's3_w1024',\n",
    "    ],\n",
    "    'model': [\n",
    "        '../input/beit-s4-w640',\n",
    "        '../input/segformer-b5-s4-w768',\n",
    "        '../input/segformer-b5-s3-w1024',\n",
    "        \n",
    "        '../input/coat-m-s3-w1024',\n",
    "    ],\n",
    "    'model_weight': [\n",
    "        0.283,\n",
    "        0.283,\n",
    "        0.283,\n",
    "        \n",
    "        0.15,\n",
    "    ],\n",
    "    \n",
    "    ## model for lung type\n",
    "    'lung_preprocessing': [\n",
    "        's4_w768',\n",
    "        's4_w768',\n",
    "    ],\n",
    "    'lung_model': [\n",
    "        '../input/effb7-w768-best',\n",
    "        '../input/effb7-w768-ds-best',\n",
    "    ],\n",
    "    'lung_model_weight': [\n",
    "        0.5,\n",
    "        0.5,\n",
    "    ],\n",
    "    \n",
    "    ## postprocessing arg\n",
    "    'TTA': True,\n",
    "    'mask_thr': {\n",
    "        'lung':           0.1,\n",
    "        'spleen':         0.2,\n",
    "        'prostate':       0.3,\n",
    "        'kidney':         0.3,\n",
    "        'largeintestine': 0.2,\n",
    "    },\n",
    "    \n",
    "    ## dataset filter for inference\n",
    "    'data_source': [\n",
    "        'HPA',\n",
    "        'Hubmap',\n",
    "    ],\n",
    "    'organ': [\n",
    "        'lung',\n",
    "        'prostate',\n",
    "        'kidney',\n",
    "        'largeintestine', \n",
    "        'spleen',\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42ea0cd",
   "metadata": {
    "papermill": {
     "duration": 0.005724,
     "end_time": "2022-09-21T13:18:13.267150",
     "exception": false,
     "start_time": "2022-09-21T13:18:13.261426",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4567a8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-21T13:18:13.280263Z",
     "iopub.status.busy": "2022-09-21T13:18:13.279994Z",
     "iopub.status.idle": "2022-09-21T13:18:13.331903Z",
     "shell.execute_reply": "2022-09-21T13:18:13.330888Z"
    },
    "papermill": {
     "duration": 0.061167,
     "end_time": "2022-09-21T13:18:13.334164",
     "exception": false,
     "start_time": "2022-09-21T13:18:13.272997",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rle</th>\n",
       "      <th>image_path</th>\n",
       "      <th>organ</th>\n",
       "      <th>pixel_size</th>\n",
       "      <th>data_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10078</td>\n",
       "      <td>12 34</td>\n",
       "      <td>../input/hubmap-organ-segmentation/test_images...</td>\n",
       "      <td>spleen</td>\n",
       "      <td>0.4945</td>\n",
       "      <td>Hubmap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    rle                                         image_path   organ  \\\n",
       "0  10078  12 34  ../input/hubmap-organ-segmentation/test_images...  spleen   \n",
       "\n",
       "   pixel_size data_source  \n",
       "0      0.4945      Hubmap  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df= pd.read_csv('../input/hubmap-organ-segmentation/test.csv')\n",
    "df= pd.read_csv('../input/hubmap-organ-segmentation/sample_submission.csv')\n",
    "for i in range(len(df)):\n",
    "    id_= df.loc[i, ['id']].values[0]\n",
    "    path= f'../input/hubmap-organ-segmentation/test_images/{id_}.tiff'\n",
    "    df.loc[i, ['image_path']]= path\n",
    "    organ= test_df[test_df['id']==int(id_)]['organ'].values[0]\n",
    "    df.loc[i, ['organ']]= organ\n",
    "    pixel_size= test_df[test_df['id']==int(id_)]['pixel_size'].values[0]\n",
    "    df.loc[i, ['pixel_size']]= pixel_size\n",
    "    data_source= test_df[test_df['id']==int(id_)]['data_source'].values[0]\n",
    "    df.loc[i, ['data_source']]= data_source\n",
    "    \n",
    "test_dataset= Customize_Dataset(df, scale= 4, img_size= 768, transforms= get_test_transform())\n",
    "test_loader_s4_w768 = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "test_dataset= Customize_Dataset(df, scale= 3, img_size= 1024, transforms= get_test_transform())\n",
    "test_loader_s3_w1024 = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "test_dataset= Customize_Dataset(df, scale= 4, img_size= 640, transforms= get_test_transform())\n",
    "test_loader_s4_w640 = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249dfd35",
   "metadata": {
    "papermill": {
     "duration": 0.006652,
     "end_time": "2022-09-21T13:18:13.347851",
     "exception": false,
     "start_time": "2022-09-21T13:18:13.341199",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "107b7bf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-21T13:18:13.362963Z",
     "iopub.status.busy": "2022-09-21T13:18:13.362065Z",
     "iopub.status.idle": "2022-09-21T13:18:13.378930Z",
     "shell.execute_reply": "2022-09-21T13:18:13.378077Z"
    },
    "papermill": {
     "duration": 0.026548,
     "end_time": "2022-09-21T13:18:13.380963",
     "exception": false,
     "start_time": "2022-09-21T13:18:13.354415",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fill_hole(m):\n",
    "    filled = m.copy()\n",
    "    pad = np.pad(m, 4)\n",
    "    lb = measure.label(pad < 0.5, background=0, connectivity=1)\n",
    "    u, cc = np.unique(lb, return_counts=True)\n",
    "    if len(u) > 2:\n",
    "        #print(u, cc)\n",
    "        lb = lb[4:-4, 4:-4]\n",
    "        for uu in u[2:]:\n",
    "            filled[lb == uu] = 1\n",
    "    return filled\n",
    "\n",
    "def rle_encode(img):\n",
    "    #the image should be transposed\n",
    "    pixels = img.T.flatten()\n",
    "    # This simplified method requires first and last pixel to be zero\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def inference(model, img, size):\n",
    "    img= img.cuda()\n",
    "    for i, m in enumerate(model):\n",
    "        with torch.no_grad():\n",
    "            m.eval()\n",
    "            if CFG['TTA']:\n",
    "                imgs= torch.cat([img, \n",
    "                                 img.rot90(1, [2,3]),\n",
    "                                 img.rot90(2, [2,3]),\n",
    "                                 img.rot90(3, [2,3]),\n",
    "                                 img.flip(-1), \n",
    "                                 img.flip(-1).rot90(1, [2,3]),\n",
    "                                 img.flip(-1).rot90(2, [2,3]),\n",
    "                                 img.flip(-1).rot90(3, [2,3])], dim=0)\n",
    "                pred= sliding_window_inference(imgs, \n",
    "                                               (size, size), \n",
    "                                               sw_batch_size= 1, \n",
    "                                               predictor= m,\n",
    "                                               mode= 'gaussian',\n",
    "                                               overlap= 0.25)\n",
    "                pred= (pred[0] + \n",
    "                       pred[1].rot90(-1, [1,2]) +\n",
    "                       pred[2].rot90(-2, [1,2]) +\n",
    "                       pred[3].rot90(-3, [1,2]) +\n",
    "                       pred[4].flip(-1) + \n",
    "                       pred[5].rot90(-1, [1,2]).flip(-1) + \n",
    "                       pred[6].rot90(-2, [1,2]).flip(-1) + \n",
    "                       pred[7].rot90(-3, [1,2]).flip(-1)) / 8\n",
    "            else:\n",
    "                pred= sliding_window_inference(img, \n",
    "                                               (size, size), \n",
    "                                               sw_batch_size= 1, \n",
    "                                               predictor= m,\n",
    "                                               mode= 'gaussian',\n",
    "                                               overlap= 0.25)[0]\n",
    "                \n",
    "        if pred.shape[0]!=1:\n",
    "            if i==0: preds= pred.softmax(dim=0)\n",
    "            else: preds+= pred.softmax(dim=0)\n",
    "        else:\n",
    "            if i==0: preds= pred.sigmoid()\n",
    "            else: preds+= pred.sigmoid()\n",
    "                \n",
    "    pred= preds/len(model)\n",
    "    pred= pred.cpu().permute(1,2,0).numpy()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee3c13f",
   "metadata": {
    "papermill": {
     "duration": 0.00585,
     "end_time": "2022-09-21T13:18:13.392947",
     "exception": false,
     "start_time": "2022-09-21T13:18:13.387097",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict 4 types organ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d76426d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-21T13:18:13.406441Z",
     "iopub.status.busy": "2022-09-21T13:18:13.406175Z",
     "iopub.status.idle": "2022-09-21T13:20:20.623182Z",
     "shell.execute_reply": "2022-09-21T13:20:20.622075Z"
    },
    "papermill": {
     "duration": 127.22688,
     "end_time": "2022-09-21T13:20:20.625996",
     "exception": false,
     "start_time": "2022-09-21T13:18:13.399116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of model: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "organs= [\n",
    "    'lung',\n",
    "    'spleen',\n",
    "    'prostate',\n",
    "    'kidney',\n",
    "    'largeintestine',\n",
    "]\n",
    "\n",
    "## load model for 4 type organ\n",
    "cv_model= []\n",
    "for i in range(len(CFG['model'])):\n",
    "    models= []\n",
    "    for m in glob.glob(CFG['model'][i]+'/**.pth'):\n",
    "        models.append( torch.load(m, map_location='cuda:0') )\n",
    "    cv_model.append(models)\n",
    "CFG['model']= cv_model\n",
    "print(f\"length of model: {len(CFG['model'])}\")\n",
    "\n",
    "indx= 0\n",
    "for i, (s4_w640_data, s4_w768_data, s3_w1024_data) in enumerate(zip(test_loader_s4_w640,\n",
    "                                                                    test_loader_s4_w768, \n",
    "                                                                    test_loader_s3_w1024)):\n",
    "    dataset={\n",
    "        's4_w640': s4_w640_data,\n",
    "        's4_w768': s4_w768_data,\n",
    "        's3_w1024': s3_w1024_data,\n",
    "    }\n",
    "    \n",
    "    for j in range(len(s4_w768_data['image'])):\n",
    "        \n",
    "        ## get img\n",
    "        organ= s4_w768_data['organ'][j]\n",
    "        if organ=='lung':\n",
    "            indx+= 1\n",
    "            continue\n",
    "        d_source= s4_w768_data['data_source'][j]\n",
    "        \n",
    "        if organ not in CFG['organ']: \n",
    "            indx+= 1\n",
    "            continue\n",
    "        if d_source not in CFG['data_source']: \n",
    "            indx+= 1\n",
    "            continue\n",
    "        \n",
    "        ## select model type\n",
    "        models= CFG['model'] if organ!='lung' else CFG['lung_model']\n",
    "        model_weight= CFG['model_weight'] if organ!='lung' else CFG['lung_model_weight']\n",
    "        \n",
    "        ## select preprocessing\n",
    "        preprocessing= CFG['preprocessing'] if organ!='lung' else CFG['lung_preprocessing']\n",
    "        \n",
    "        ## inference\n",
    "        for k, m in enumerate(models):\n",
    "                                                  \n",
    "            ## get data\n",
    "            data= dataset[ preprocessing[k] ]\n",
    "            window_size= int(preprocessing[k].split('_')[-1][1:])\n",
    "            img= data['image'][j]\n",
    "            img= torch.unsqueeze(img, dim= 0)\n",
    "            ori_shape= data['ori_shape'][j].numpy()\n",
    "            pad= data['pad'][j]\n",
    "            \n",
    "            mask= inference(m, img, window_size)\n",
    "            if mask.shape[2]!=1:\n",
    "                mask= mask[..., organs.index(organ)+1]\n",
    "            \n",
    "            ## if pad, recover\n",
    "            if pad!=0:\n",
    "                aug= get_crop_transform(pad)\n",
    "                mask= aug(image= mask)['image']\n",
    "                mask= cv2.resize(mask, tuple(ori_shape))\n",
    "            else:\n",
    "                mask= cv2.resize(mask, tuple(ori_shape))\n",
    "            \n",
    "            if k==0: masks= mask * model_weight[k]\n",
    "            else: masks+= mask * model_weight[k]\n",
    "                \n",
    "        mask= masks\n",
    "        if d_source=='HPA' and organ!='lung':\n",
    "            mask[ mask>=0.5 ]= 1\n",
    "            mask[ mask<0.5 ]= 0\n",
    "        else:\n",
    "            mask[ mask>=CFG['mask_thr'][organ] ]= 1\n",
    "            mask[ mask<CFG['mask_thr'][organ] ]= 0\n",
    "        mask= mask.astype(np.uint8)\n",
    "        \n",
    "        ## fill mask hole\n",
    "        if organ in ['spleen']:\n",
    "            mask= fill_hole(mask)\n",
    "        \n",
    "#         plt.imshow(img[0].permute(1,2,0).numpy())\n",
    "#         plt.show()\n",
    "#         plt.imshow(mask)\n",
    "#         plt.show()\n",
    "\n",
    "        rle= rle_encode(mask)\n",
    "        df.loc[indx, 'rle']= rle\n",
    "        indx+= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e25979",
   "metadata": {
    "papermill": {
     "duration": 0.006713,
     "end_time": "2022-09-21T13:20:20.639835",
     "exception": false,
     "start_time": "2022-09-21T13:20:20.633122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict lung type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cd6c380",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-21T13:20:20.655658Z",
     "iopub.status.busy": "2022-09-21T13:20:20.655330Z",
     "iopub.status.idle": "2022-09-21T13:20:45.871468Z",
     "shell.execute_reply": "2022-09-21T13:20:45.870243Z"
    },
    "papermill": {
     "duration": 25.228029,
     "end_time": "2022-09-21T13:20:45.874729",
     "exception": false,
     "start_time": "2022-09-21T13:20:20.646700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of lung_model: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "del CFG['model']\n",
    "\n",
    "organs= [\n",
    "    'lung',\n",
    "    'spleen',\n",
    "    'prostate',\n",
    "    'kidney',\n",
    "    'largeintestine',\n",
    "]\n",
    "\n",
    "## load model for lung type\n",
    "cv_model= []\n",
    "for i in range(len(CFG['lung_model'])):\n",
    "    models= []\n",
    "    for m in glob.glob(CFG['lung_model'][i]+'/**.pth'):\n",
    "        models.append( torch.load(m, map_location='cuda:0') )\n",
    "    cv_model.append(models)\n",
    "CFG['lung_model']= cv_model\n",
    "print(f\"length of lung_model: {len(CFG['lung_model'])}\")\n",
    "\n",
    "indx= 0\n",
    "for i, (s4_w640_data, s4_w768_data, s3_w1024_data) in enumerate(zip(test_loader_s4_w640,\n",
    "                                                                    test_loader_s4_w768, \n",
    "                                                                    test_loader_s3_w1024)):\n",
    "    dataset={\n",
    "        's4_w640': s4_w640_data,\n",
    "        's4_w768': s4_w768_data,\n",
    "        's3_w1024': s3_w1024_data,\n",
    "    }\n",
    "    \n",
    "    for j in range(len(s4_w768_data['image'])):\n",
    "        \n",
    "        ## get img\n",
    "        organ= s4_w768_data['organ'][j]\n",
    "        if organ!='lung':\n",
    "            indx+= 1\n",
    "            continue\n",
    "        d_source= s4_w768_data['data_source'][j]\n",
    "        \n",
    "        if organ not in CFG['organ']: \n",
    "            indx+= 1\n",
    "            continue\n",
    "        if d_source not in CFG['data_source']: \n",
    "            indx+= 1\n",
    "            continue\n",
    "        \n",
    "        ## select model type\n",
    "        models= CFG['model'] if organ!='lung' else CFG['lung_model']\n",
    "        model_weight= CFG['model_weight'] if organ!='lung' else CFG['lung_model_weight']\n",
    "        \n",
    "        ## select preprocessing\n",
    "        preprocessing= CFG['preprocessing'] if organ!='lung' else CFG['lung_preprocessing']\n",
    "        \n",
    "        ## inference\n",
    "        for k, m in enumerate(models):\n",
    "                                                  \n",
    "            ## get data\n",
    "            data= dataset[ preprocessing[k] ]\n",
    "            window_size= int(preprocessing[k].split('_')[-1][1:])\n",
    "            img= data['image'][j]\n",
    "            img= torch.unsqueeze(img, dim= 0)\n",
    "            ori_shape= data['ori_shape'][j].numpy()\n",
    "            pad= data['pad'][j]\n",
    "            \n",
    "            mask= inference(m, img, window_size)\n",
    "            if mask.shape[2]!=1:\n",
    "                mask= mask[..., organs.index(organ)+1]\n",
    "            \n",
    "            ## if pad, recover\n",
    "            if pad!=0:\n",
    "                aug= get_crop_transform(pad)\n",
    "                mask= aug(image= mask)['image']\n",
    "                mask= cv2.resize(mask, tuple(ori_shape))\n",
    "            else:\n",
    "                mask= cv2.resize(mask, tuple(ori_shape))\n",
    "            \n",
    "            if k==0: masks= mask * model_weight[k]\n",
    "            else: masks+= mask * model_weight[k]\n",
    "                \n",
    "        mask= masks\n",
    "        if d_source=='HPA' and organ!='lung':\n",
    "            mask[ mask>=0.5 ]= 1\n",
    "            mask[ mask<0.5 ]= 0\n",
    "        else:\n",
    "            mask[ mask>=CFG['mask_thr'][organ] ]= 1\n",
    "            mask[ mask<CFG['mask_thr'][organ] ]= 0\n",
    "        mask= mask.astype(np.uint8)\n",
    "        \n",
    "        ## fill mask hole\n",
    "        if organ in ['lung']:\n",
    "            mask= fill_hole(mask)\n",
    "\n",
    "        rle= rle_encode(mask)\n",
    "        df.loc[indx, 'rle']= rle\n",
    "        indx+= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a798f9",
   "metadata": {
    "papermill": {
     "duration": 0.00615,
     "end_time": "2022-09-21T13:20:45.888302",
     "exception": false,
     "start_time": "2022-09-21T13:20:45.882152",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62277d91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-21T13:20:45.901875Z",
     "iopub.status.busy": "2022-09-21T13:20:45.901557Z",
     "iopub.status.idle": "2022-09-21T13:20:45.923550Z",
     "shell.execute_reply": "2022-09-21T13:20:45.922415Z"
    },
    "papermill": {
     "duration": 0.031162,
     "end_time": "2022-09-21T13:20:45.925636",
     "exception": false,
     "start_time": "2022-09-21T13:20:45.894474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10078</td>\n",
       "      <td>4476 71 6480 99 8496 115 10516 120 12537 123 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                                rle\n",
       "0  10078  4476 71 6480 99 8496 115 10516 120 12537 123 1..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= df.drop(['image_path'], axis=1)\n",
    "df= df.drop(['organ'], axis=1)\n",
    "df= df.drop(['pixel_size'], axis=1)\n",
    "df= df.drop(['data_source'], axis=1)\n",
    "df.to_csv('submission.csv', index=False)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 449.444019,
   "end_time": "2022-09-21T13:20:49.357340",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-09-21T13:13:19.913321",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
